# Example: LLM configuration with budget caps
# Use this for LLM API proxying with cost control

targets:
  # OpenAI with budget caps
  openai:
    base_url: "https://api.openai.com/v1"
    timeout_ms: 20000
    
    # Circuit breaker
    circuit:
      error_threshold: 5
      cooldown_s: 60
    
    # LLM-specific settings
    llm:
      provider: "openai"
      default_model: "gpt-4o-mini"
      max_tokens: 1024
      temperature: 0.7
      
      # Budget control: predictable costs
      soft_cost_cap_usd: 0.01    # Throttle if exceeded (reduces max_tokens)
      hard_cost_cap_usd: 0.05    # Reject if exceeded
    
    # Cache: LLM responses cached for 1 hour
    cache:
      ttl_s: 3600
      enabled: true
    
    # Auth
    auth:
      type: bearer_env
      env_var: OPENAI_API_KEY
    
    # Fallback chain: try Anthropic if OpenAI fails
    fallback_targets: ["anthropic"]
    
    # Retry matrix
    retry_matrix:
      "429":
        attempts: 3
        backoff: "exp-jitter"
        base_s: 1.0
        max_s: 60.0
      "5xx":
        attempts: 2
        backoff: "exp-jitter"
        base_s: 1.0
      "net":
        attempts: 2
        backoff: "exp-jitter"
        base_s: 1.0

  # Anthropic as fallback
  anthropic:
    base_url: "https://api.anthropic.com/v1"
    timeout_ms: 25000
    
    circuit:
      error_threshold: 5
      cooldown_s: 60
    
    llm:
      provider: "anthropic"
      default_model: "claude-3-haiku-20240307"
      max_tokens: 2048
      temperature: 0.2
      
      # Budget control
      soft_cost_cap_usd: 0.005
      hard_cost_cap_usd: 0.02
    
    cache:
      ttl_s: 300
      enabled: true
    
    auth:
      type: bearer_env
      env_var: ANTHROPIC_API_KEY
    
    retry_matrix:
      "429":
        attempts: 3
        backoff: "exp-jitter"
        base_s: 2.0
      "5xx":
        attempts: 2
        backoff: "exp-jitter"
        base_s: 1.0
      "net":
        attempts: 2
        backoff: "exp-jitter"
        base_s: 1.0

